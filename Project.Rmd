---
title: "Project of Data Science Course"
author: "ZZG"
date: "7/28/2017"
output: html_document
---
```{r package_options, include=FALSE,cache=F}
knitr::opts_chunk$set(echo = T,warning = F,message = F)
```

## Part1: Exploratory Data Analysis
```{r}
# Read data in and check the summary
library(caret);library(tidyverse)
setwd("~/Downloads/Coursera/Practical Machine Learning_John Hopkings")
training <- read.csv("pml-training.csv")[-1]
testing <- read.csv("pml-testing.csv")[-1]
#summary(training)

# In summary we can see lots of predictors are basicly blank or invalid
# So the feature size is reduced to contain more valid information
# In future we can compare the training performance between the full data and reduced data
trainingReduced <- training[,c(1:10,36:48,59:67,83:85,101,112:123,139,150:159)]
summary(trainingReduced)

# I noticed the variable "new_window" has largely imbalanced data, so delete
trainingReduced <- trainingReduced[,-5]

testingReduced <- testing[,which(names(testing) %in% names(trainingReduced))]

# Plot the user distribution and show their exercise manner
qplot(user_name,data=trainingReduced,fill=classe,main = "Histogram of Users by Classes")


# Use featurePlot() to see the relations between several pairs of variables
# We can see some correlation between certain pairs
# However we do not want to remove any variable carelessly
featurePlot(trainingReduced[,5:10],trainingReduced$classe,"pairs")
#featurePlot(trainingReduced[,25:35],trainingReduced$classe,"pairs")
```
  
###Comments:  
1) Fitting models are more complicated than it looks. We should pre-process variables on missing values, scales and other problems.  
2) If certain predictor has too many missing values, it won't contribute to our model, that's why I deleted quite a few variables here to make the dataset more efficient.  
3) Try various way to explore the data is also crucial. It will help us to choose certain variables, deal with correlation if there's any, and explain our final model with more sense.  



## Part2: Model Fitting
```{r}
set.seed(0818)
inTrain <- createDataPartition(y=trainingReduced$classe,p=0.7,list=F)
train <- trainingReduced[inTrain,]
test <- trainingReduced[-inTrain,]

# First I want to try tree model to this classification problem
# The result is not satisfying, maybe because our data is not highly categorical
modTree <- train(classe ~.,data=train, method='rpart')
treePredTest <- predict(modTree,test)
table(treePredTest,test$classe)
errRateTree <- sum(treePredTest != test$classe)/nrow(test)
errRateTree

# Then we can try LDA model to classify
modLda <- train(classe ~.,data=train, method='lda')
ldaPredTest <- predict(modLda,test)
table(ldaPredTest,test$classe)
errRateLda <- sum(ldaPredTest != test$classe)/nrow(test)
errRateLda

# Remember we haven't seen clear correlation between variables
# So we can try naive bayes model to classify
# From the result, seems the independence assumption may not hold perfectly
# Compare to LDA, the model is not doing as well
modNb <- train(classe ~.,data=train, method='naive_bayes')
nbPredTest <- predict(modNb,test)
table(nbPredTest,test$classe)
errRateNb <- sum(nbPredTest != test$classe)/nrow(test)
errRateNb

# Now I will try KNN method
# I use class package to run the algorithm
# Using KNN method needs several pre-processing
# I change important factor variables into dummy variables
# Also I center and scale predictors
library(class)
userTrain <- matrix(0,nrow=nrow(train),ncol=6)
colnames(userTrain) <- unique(train$user_name)
for (row in 1:nrow(train)){
  userTrain[row,which(train$user_name[row] == colnames(userTrain))] <- 1
}
userTrain <- userTrain[,-6]

userTest <- matrix(0,nrow=nrow(test),ncol=5)
colnames(userTest) <- colnames(userTrain)
for (row in 1:nrow(test)){
  userTest[row,which(test$user_name[row] == colnames(userTest))] <- 1
}

#knnPred <- knn(train[,-c(1,4,58)],test[,-c(1,4,58)],train[,58],k=15)
#table(modKnn,test$classe)
#errRateKnn <- sum(modKnn != test$classe)/nrow(test)
#errRateKnn

trainPreProcess <- scale(cbind(userTrain,train[-c(1,4,58)]), center = TRUE, scale = TRUE)
testPreProcess <- scale(cbind(userTest,test[,-c(1,4,58)]), center = TRUE, scale = TRUE)

knnPredScale <- knn(trainPreProcess,testPreProcess,train[,58],k=25)
errRateKnn <- sum(knnPredScale != test$classe)/nrow(test)
errRateKnn

# Lastly we can try SVM model to classify
# We use all the predictors in our data
library(e1071)
modSvm <- svm(classe ~.,data=train[,-4])
svmPredTest <- predict(modSvm,test)
table(svmPredTest,test$classe)
errRateSvm <- sum(svmPredTest != test$classe)/nrow(test)
errRateSvm

errRates <- data.frame("Tree"=errRateTree,"LDA"=errRateLda,"NaiveBayes"=errRateNb,"KNN"=errRateKnn,"SVM"=errRateSvm)
errRates
```
  
###Comments:  
1) Here I choose 5 models to train my data: Tree, LDA, Naive Bayes, KNN, and SVM.  
2) For certain model, such as KNN, we should pre-process the data by ourselves due to the design of the package(pacakge "class"). If we don't scale and center the training data, and the testing data, the error rate will be extremely high, which is almost like random guess.  
3) From all 5 models, we can see SVM performs the best, and Tree performs the worst. Go back to my data, most of the variables are numerical, while Tree model is more efficient on categorical variables.  
4) Here I didn't use any cross-validation besides the default settings, I will use different parameter settings in next part to compare the results.  


## Part3: Cross Validation
```{r}
# Tree model trained with CV
treeTrainControl<- trainControl(method="LGOCV", repeats=5, p=0.7)
modTreeCv <- train(classe ~.,data=train, method='rpart',trControl = treeTrainControl)
table(predict(modTreeCv,test),test$classe)
errRateTreeCv <- sum(predict(modTreeCv,test) != test$classe)/nrow(test)
errRateTreeCv

# Then we try LDA with different cross validation methods
ldaTrainControl<- trainControl(method="cv", number=25, p=0.8)
modLdaCv <- train(classe ~.,data=train, method='lda',trControl = ldaTrainControl)
table(predict(modLdaCv,test),test$classe)
errRateLdaCv <- sum(predict(modLdaCv,test) != test$classe)/nrow(test)
errRateLdaCv

# Then we change the cross validation methods for Naive Bayes
nbTrainControl<- trainControl(method="boot", number=25, p=0.8)
modNbCv <- train(classe ~.,data=train, method='naive_bayes',trControl = nbTrainControl)
table(predict(modNbCv,test),test$classe)
errRateNbCv <- sum(predict(modNbCv,test) != test$classe)/nrow(test)
errRateNbCv

# Then we use LOOCV cross validation methods for KNN 
knnPredScaleCv <- knn.cv(trainPreProcess,train[,58],k=25)
errRateKnnCv <- sum(knnPredScaleCv != train$classe)/nrow(train)
errRateKnnCv

# Lastly we can tune SVM model
tune <- tune.control(sampling = 'cross',cross=10)
modSvmCv <- tune.svm(classe ~.,data=train[,-4], tunecontrol = tune)
table(predict(modSvmCv$best.model,test),test$classe)
errRateSvmCv <- sum(predict(modSvmCv$best.model,test) != test$classe)/nrow(test)
errRateSvmCv

# Now we bag all the methods to predict
vote <- function(input){
  input <- as.data.frame(table(input))
  inputSorted <- arrange(input,desc(Freq))
  return(inputSorted[1,1])
}

resultTest <- data.frame("TreeResults"=treePredTest,"LdaResults"=ldaPredTest,"NaiveBayesResults"=nbPredTest,"KnnResults"=knnPredScale,"SvmResults"=svmPredTest)

predResultTest <- apply(resultTest,1,vote)
sum(predResultTest == test$classe)/length(predResultTest)

# Consolidate all error rates
data.frame("Before CV"=c(errRateTree,errRateLda,errRateNb,errRateKnn,errRateSvm),"After CV"=c(errRateTreeCv,errRateLdaCv,errRateNbCv,errRateKnnCv,errRateSvmCv))
```
  
###Comment:    
1) In Part2 and this Part, I both splitted the origianl training file into training data and testing data to test the predicting power.    
2) As we say, different cross validation methods have no effects on most of my cases. KNN method show minor difference between the original algorithm and tuned algorithm, however, we can see the default tuning methods built in R packages are already doing a reasonable job.  
3) In addition, in this part I also use the bagged algorithm to consolidate all methods, and to see whether the accuracy would beat the best one of them. However, since our models varied a lot on their performances, the combined accuracy still cannot top SVM, which has around 95% accuracy.   


## Part4: Predict on testing dataset
```{r}
# Predict on the 20 testing data
treePred <- predict(modTree,newdata = testingReduced)
treePred
ldaPred <- predict(modLda,newdata = testingReduced)
ldaPred
nbPred <- predict(modNb,newdata = testingReduced)
nbPred
knnTestPreProcess <- scale(testingReduced[,-c(1,4)], center = TRUE, scale = TRUE)
knnPred <- knn(trainPreProcess[,-c(1:5)],knnTestPreProcess,train[,58],k=25)
knnPred
svmPred <- predict(modSvm,newdata = testingReduced[,-4])
svmPred

# Consolidate the results
predResult <- data.frame("Tree"=treePred,"LDA"=ldaPred,"Naive Bayes"=nbPred,"KNN"=knnPred,"SVM"=svmPred)

# Calculate the vote
voteResult <- apply(predResult,1,vote)
voteResult

table(voteResult,svmPred)
sum(voteResult == svmPred)/length(voteResult)
sum(voteResult == ldaPred)/length(voteResult)
```
    
###Comments:  
Here we use all 5 models and bagged them to vote, and we get the final classification, which is quite similar to SVM model.






